# Rendering a Hologram* for Looking Glass Displays

Now that we are rendering Gaussian Splats in a 2D window, how can we extend this to draw to a 3D display like the Looking Glass displays?

*Um... Actually! If you want to be pedantic [this](https://en.wikipedia.org/wiki/Holography#:~:text=A%20hologram%20is%20a%20recording,form%20of%20an%20interference%20pattern.) is the only thing you can call a hologram. 

## How we see 3D.

First an overview of stereo vision. Most humans (and animals for that matter) have two eyes that allow us to see two slightly different perspectives at the same time. Our brains then process both of those slightly different views, as well as other [depth queues](https://pressbooks.umn.edu/sensationandperception/chapter/oculomotor-and-monocular-depth-cues/) to generate a 3D representation of the world around us. As an example, this is why you can very likely see that the device you are reading this on, is in front of the stuff behind it.

Most people are familiar with two types of 3D technologies. A traditional VR headset and the [REALD 3D](https://en.wikipedia.org/wiki/RealD_3D) glasses you may have worn in the cinema, work by sending 2 slightly different images to the viewer, one image made for each eye. In the case of a VR headset this is done using 2 lenses and 2 screens (one set per eye) and by tracking the users head motions. In the cinema 2 polarized projectors are used to project two images at once onto the screen and then the glasses select which image each eye sees using a pair of polarized filters. 

Projecting just two images has some fundamental flaws. First, as the user is not tracked it is not possible to correct for the offset of the viewers position to the center of the screen. This is why you may feel a bit sick if you sit far away from the center of the cinema. Second, and similarly, by only having two views you really only have a single location where the image looks correct.

In VR the user is tracked, but this has its own flaws. The tracking has gotten VERY good, but it is still far from perfect, and the mismatch between the users own spatial awareness and the views generated by the renderer can cause vr sickness in some users.

## How Looking Glass Displays work.

Looking Glass displays work differently. Instead of sending out only two views a Looking Glass display sends out 50 - 150 ish views depending on the rendering settings. These views are spread across a view cone of around 60 ish degrees in front of the display. As the user looks at the display and moves around both eyes seamlessly transition between views creating the illusion of a 3d scene in a volume around the display.

The Looking Glass Bridge software will automatically do the hard bits of managing a window and appropriately generating the device specific image that is needed to be shown on the display, all we need to do is capture the scene from multiple views and pass that on to Bridge.

In this repo we use the Bridge In Proc renderer to render in real time to a Looking Glass Display. The InProc SDK as I guess you would call it, is currently in testing and included with bridge installations. Bridge also has a rest api that can be used to send images and videos to Bridge for rendering. For more info on the rest api see [Bridge-js](https://github.com/Looking-Glass/bridge.js/) or [LKG-Toolkit](https://github.com/Looking-Glass/LKG-Toolkit).

Ok the small amount of shilling I can be bothered to write is over, lets get to the code:

# How to render for a Looking Glass Display using Bridge InProc

We need 3 things to render for a Looking Glass Display:

1. Some way to store all the views we need.
2. Some way to capture all the views.
3. Some way to display those views on the Looking Glass Display.

Luckily step 3 is done via Bridge so we only need to worry about step 1, and step 2.

## Step 1. How we store a hologram in a 2D texture?

First we need to talk about how we will store all of the views. Bridge expects a texture pointer to what we refer to as a Quilt. This is essentially a single image that stores all the views as tiles in a 2d grid. 

Below is a simplified example of how to implement a quilt struct to deal with the math in a quilt, the Quilt class in this renderer is a bit more complex as it also deals with the camera math we need to position the views, I will omit that for this section.

[CODE](../../Quilt.cs)

```csharp
public class Quilt
{
    public uint QuiltWidth = 4096;
    public uint QuiltHeight = 4096;
    
    public uint TileCountX = 6;
    public uint TileCountY = 8;
    public uint TileCount = 48;
    
    public uint TileWidth = TileCountX / QuiltWidth;
    public uint TileHeight = TileCountY / QuiltHeight;

    // input: View index between 0 and TileCount
    // output: the top left position of the tile inside the quilt texture
    // you would use this function like this in OpenGL:
    // GL.Viewport(xOffset, yOffset, (int)TileWidth, (int)TileHeight);
    public (int xOffset, int yOffset) GetTilePosition(int view)
    {
        // Calculate the position of the view within the quilt
        uint rowIndex = (uint)index / TileCountX;
        uint columnIndex = (uint)index % TileCountX;

        // Due to legacy reasons we flip the y index of the views
        rowIndex = TileCountY - 1 - rowIndex;

        // Calculate the viewport position
        int viewportX = (int)(columnIndex * TileWidth);
        int viewportY = (int)(rowIndex * TileHeight);

        return (viewportX, viewportY);
    }
}
```

If you would like a less code heavy explanation of quilts the [Looking Glass docs](https://docs.lookingglassfactory.com/keyconcepts/quilts) are pretty good.

# Step 2. How to capture a scene
Ok, now that we have a method for storing a quilt, how do we capture?. If you look at the OnRenderFrame function of the window class you will see something like this:

### The lie from last time.

If you remember from the last [section](2DRendering.md) we set the framebuffer and render using code that looks a bit like this:
```csharp
// Get the camera matrices 
viewMatrix = camera.GetViewMatrix();
projectionMatrix = camera.GetProjectionMatrix();

// bind the window framebuffer
// we do this every frame to support the hologram rendering
GL.BindFramebuffer(FramebufferTarget.Framebuffer, 0);

// clear framebuffer
GL.Clear(ClearBufferMask.ColorBufferBit);

// we also set the viewport every frame to support the hologram rendering
GL.Viewport(0, 0, Size.X, Size.Y);

// ###################### void render() #############################
// in the actual code this is abstracted out into a function but I removed
// the indirection to make the code easier to follow

// set shader uniforms
shader.SetMatrix4("view_matrix", viewMatrix);
shader.SetVector3("cam_pos", camera.Position);
shader.SetMatrix4("projection_matrix", projectionMatrix);

// sort indices array using fast gpu accelerated radix sort using CUDA
// this will get its own doc.
sorter.sort(viewMatrix);

// update data in OpenGL indices buffer
GL.BindBufferBase(BufferRangeTarget.ShaderStorageBuffer, 1, sorter.cudaGlInteropIndexBuffer.glBufferHandle);

// draw each splat using standard instanced rendering
GL.DrawElementsInstanced(PrimitiveType.Triangles, 6, DrawElementsType.UnsignedInt, 0, gaussians.Length);

// ###################### end void render() #############################

SwapBuffers();
```

This was a small lie, as noted in the comments we actually abstract out the draw step to its own function. We do this so that we can pass that render function to the quilt class which then does the quilt rendering for us. You can see the real code below:

```csharp
private void render(Matrix4 viewMatrix, Matrix4 projectionMatrix)
{
    shader.SetMatrix4("view_matrix", viewMatrix);
    shader.SetVector3("cam_pos", camera.Position);
    shader.SetMatrix4("projection_matrix", projectionMatrix);

    if(!sortOnce)
    {
        sorter.sort(viewMatrix);
    }

    GL.BindBufferBase(BufferRangeTarget.ShaderStorageBuffer, 1, sorter.cudaGlInteropIndexBuffer.glBufferHandle);
    GL.DrawElementsInstanced(PrimitiveType.Triangles, 6, DrawElementsType.UnsignedInt, 0, gaussians.Length);
}

protected override void OnRenderFrame(FrameEventArgs args)
{
    base.OnRenderFrame(args);

    Matrix4 viewMatrix;
    Matrix4 projectionMatrix;

    if(sortOnce)
    {
        viewMatrix = camera.GetViewMatrix();
        sorter.sort(viewMatrix);
    }

    shader.Use();
    GL.BindVertexArray(vao);

    // draw 3d
    if (quilt != null && quilt.IsValid)
    {
        quilt.Draw(render, camera, depthiness, focus);
    }

    // draw 2d
    viewMatrix = camera.GetViewMatrix();
    projectionMatrix = camera.GetProjectionMatrix();

    GL.BindFramebuffer(FramebufferTarget.Framebuffer, 0);

    GL.Clear(ClearBufferMask.ColorBufferBit);
    GL.Viewport(0, 0, Size.X, Size.Y);

    render(viewMatrix, projectionMatrix);

    SwapBuffers();

    fpsCounter.Update(args.Time);
}
```
[CODE](../../Graphics/Window.cs)

After we sort the splats we first render the quilt using the Quilt.Draw function, then we set the framebuffer back to the 2d framebuffer and render to that window.

### Quilt.Draw

This is where the magic happens.

```csharp
public void Draw(Action<Matrix4, Matrix4> render, Camera camera, float depthiness, float focus)
{
    // set the quilt framebuffer as the target to draw to
    GL.BindFramebuffer(FramebufferTarget.Framebuffer, QuiltRenderFBO);

    // clear
    GL.Clear(ClearBufferMask.ColorBufferBit);

    // for each view
    for (int i = 0; i < TileCount; i++)
    {
        // set the viewport we are drawing to to the correct one for this view
        SetViewport(i);

        // calculate the view and projection matrices for that view
        (Matrix4 viewMatrix, Matrix4 projectionMatrix) = ComputeViewCameraData(camera, i, true, depthiness, focus);

        // call the render function we pass in from the Window class to render to the splats
        render(viewMatrix, projectionMatrix);
    }

    // this tells bridge to render a new frame and swap the framebuffer
    BridgeInProc.Controller.DrawInteropQuiltTextureGL(
        window, (ulong)QuiltRenderTextureID, BridgeInProc.PixelFormats.RGBA,
        QuiltWidth, QuiltHeight,
        TileCountX, TileCountY, DesiredViewAspect, 1.0f);
}
```

For this first section we will ignore the mathy bit that is ComputeViewCameraData.

The Draw function sets the quilt framebuffer as the target